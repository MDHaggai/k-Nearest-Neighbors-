{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84712cc",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (k-NN) and Instance-Based Learning\n",
    "\n",
    "## Overview\n",
    "This notebook provides a comprehensive analysis of the k-Nearest Neighbors algorithm, including:\n",
    "- Algorithm intuition and mathematical foundation\n",
    "- Implementation from scratch\n",
    "- Testing on real datasets\n",
    "- Comparison with other models\n",
    "- Analysis of lazy learning concepts\n",
    "- Distance metrics evaluation\n",
    "- Performance on high-dimensional datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdff685",
   "metadata": {},
   "source": [
    "## 1. Algorithm Intuition and Mathematics\n",
    "\n",
    "### What is k-NN?\n",
    "k-Nearest Neighbors is a **lazy learning** algorithm that:\n",
    "- Stores all training data during the training phase\n",
    "- Makes predictions based on the k closest training examples\n",
    "- Uses majority voting for classification\n",
    "- Uses averaging for regression\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "**Distance Metrics:**\n",
    "1. **Euclidean Distance**: $d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$\n",
    "2. **Manhattan Distance**: $d(x,y) = \\sum_{i=1}^{n}|x_i - y_i|$\n",
    "3. **Cosine Distance**: $d(x,y) = 1 - \\frac{x \\cdot y}{||x|| \\cdot ||y||}$\n",
    "\n",
    "**Prediction Formula:**\n",
    "For classification: $\\hat{y} = \\text{mode}(y_{k-neighbors})$\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Lazy Learning**: No explicit training phase\n",
    "- **Instance-based**: Uses specific instances for prediction\n",
    "- **Non-parametric**: Makes no assumptions about data distribution\n",
    "- **Curse of Dimensionality**: Performance degrades in high dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19278e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_wine, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom k-NN implementation\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from knn_implementation import KNearestNeighbors, load_and_prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c15cc",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72416606",
   "metadata": {},
   "source": [
    "## 2. Real Dataset Analysis\n",
    "\n",
    "Now let's load and analyze the real datasets we downloaded from various sources. These datasets will help us understand how k-NN performs on different types of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d06b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the downloaded real datasets\n",
    "import os\n",
    "\n",
    "# Check if data directory exists and list available datasets\n",
    "data_dir = '../data'\n",
    "if os.path.exists(data_dir):\n",
    "    print(\"Available datasets:\")\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"  - {file}\")\n",
    "else:\n",
    "    print(\"Data directory not found. Please run the dataset downloader first.\")\n",
    "\n",
    "# Load each dataset\n",
    "real_datasets = {}\n",
    "\n",
    "try:\n",
    "    # Banknote Authentication Dataset\n",
    "    banknote_df = pd.read_csv('../data/banknote_authentication.csv')\n",
    "    real_datasets['Banknote Authentication'] = {\n",
    "        'data': banknote_df,\n",
    "        'target_col': 'class',\n",
    "        'description': 'Distinguish genuine vs forged banknotes based on image features'\n",
    "    }\n",
    "    print(f\"\\nBanknote Authentication: {banknote_df.shape[0]} samples, {banknote_df.shape[1]-1} features\")\n",
    "    \n",
    "    # Glass Identification Dataset\n",
    "    glass_df = pd.read_csv('../data/glass_identification.csv')\n",
    "    real_datasets['Glass Identification'] = {\n",
    "        'data': glass_df,\n",
    "        'target_col': 'glass_type',\n",
    "        'description': 'Classify glass types based on chemical composition'\n",
    "    }\n",
    "    print(f\"Glass Identification: {glass_df.shape[0]} samples, {glass_df.shape[1]-1} features\")\n",
    "    \n",
    "    # Ionosphere Dataset\n",
    "    ionosphere_df = pd.read_csv('../data/ionosphere.csv')\n",
    "    real_datasets['Ionosphere'] = {\n",
    "        'data': ionosphere_df,\n",
    "        'target_col': 'class',\n",
    "        'description': 'Radar data classification (good vs bad returns)'\n",
    "    }\n",
    "    print(f\"Ionosphere: {ionosphere_df.shape[0]} samples, {ionosphere_df.shape[1]-1} features\")\n",
    "    \n",
    "    # Synthetic High-Dimensional Dataset\n",
    "    synthetic_df = pd.read_csv('../data/synthetic_high_dim.csv')\n",
    "    real_datasets['Synthetic High-Dim'] = {\n",
    "        'data': synthetic_df,\n",
    "        'target_col': 'target',\n",
    "        'description': 'High-dimensional synthetic data for curse of dimensionality analysis'\n",
    "    }\n",
    "    print(f\"Synthetic High-Dim: {synthetic_df.shape[0]} samples, {synthetic_df.shape[1]-1} features\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading datasets: {e}\")\n",
    "    print(\"Please ensure the dataset downloader has been run successfully.\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(real_datasets)} real datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset characteristics\n",
    "if real_datasets:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Real Datasets Overview', fontsize=16)\n",
    "    \n",
    "    dataset_names = list(real_datasets.keys())\n",
    "    \n",
    "    for i, (name, dataset_info) in enumerate(real_datasets.items()):\n",
    "        if i >= 4:  # Only plot first 4 datasets\n",
    "            break\n",
    "            \n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        \n",
    "        df = dataset_info['data']\n",
    "        target_col = dataset_info['target_col']\n",
    "        \n",
    "        # Plot class distribution\n",
    "        class_counts = df[target_col].value_counts()\n",
    "        axes[row, col].bar(range(len(class_counts)), class_counts.values)\n",
    "        axes[row, col].set_title(f'{name}\\nClass Distribution')\n",
    "        axes[row, col].set_xlabel('Class')\n",
    "        axes[row, col].set_ylabel('Count')\n",
    "        axes[row, col].set_xticks(range(len(class_counts)))\n",
    "        axes[row, col].set_xticklabels(class_counts.index)\n",
    "        \n",
    "        # Add dataset info as text\n",
    "        info_text = f\"Samples: {df.shape[0]}\\nFeatures: {df.shape[1]-1}\\nClasses: {len(class_counts)}\"\n",
    "        axes[row, col].text(0.02, 0.98, info_text, transform=axes[row, col].transAxes,\n",
    "                           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\nDetailed Dataset Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    for name, dataset_info in real_datasets.items():\n",
    "        df = dataset_info['data']\n",
    "        target_col = dataset_info['target_col']\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Description: {dataset_info['description']}\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        print(f\"  Target distribution: {dict(df[target_col].value_counts())}\")\n",
    "        print(f\"  Missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad440d0a",
   "metadata": {},
   "source": [
    "## 3. Comprehensive k-NN Evaluation on Real Datasets\n",
    "\n",
    "Let's evaluate our k-NN implementation on all the real datasets and compare performance across different data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e01657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset preparation function\n",
    "sys.path.append('../src')\n",
    "from dataset_downloader import prepare_dataset_for_knn\n",
    "\n",
    "# Evaluate k-NN on all real datasets\n",
    "real_dataset_results = {}\n",
    "print(\"k-NN Evaluation on Real Datasets\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, dataset_info in real_datasets.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    \n",
    "    df = dataset_info['data']\n",
    "    target_col = dataset_info['target_col']\n",
    "    \n",
    "    try:\n",
    "        # Prepare data\n",
    "        X_train, X_test, y_train, y_test, feature_names = prepare_dataset_for_knn(\n",
    "            df, target_col, test_size=0.3\n",
    "        )\n",
    "        \n",
    "        # Test different k values\n",
    "        k_values = range(1, 16)\n",
    "        accuracies = []\n",
    "        \n",
    "        for k in k_values:\n",
    "            # Use our custom k-NN implementation\n",
    "            knn = KNearestNeighbors(k=k)\n",
    "            knn.fit(X_train, y_train)\n",
    "            y_pred = knn.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            accuracies.append(acc)\n",
    "        \n",
    "        # Store results\n",
    "        best_k = k_values[np.argmax(accuracies)]\n",
    "        best_accuracy = max(accuracies)\n",
    "        \n",
    "        real_dataset_results[name] = {\n",
    "            'k_values': k_values,\n",
    "            'accuracies': accuracies,\n",
    "            'best_k': best_k,\n",
    "            'best_accuracy': best_accuracy,\n",
    "            'n_samples': len(df),\n",
    "            'n_features': len(feature_names)\n",
    "        }\n",
    "        \n",
    "        print(f\"  Best k: {best_k}, Best accuracy: {best_accuracy:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {name}: {e}\")\n",
    "\n",
    "print(f\"\\nEvaluation completed for {len(real_dataset_results)} datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize k-NN performance across all real datasets\n",
    "if real_dataset_results:\n",
    "    # Plot 1: k vs Accuracy for all datasets\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Subplot 1: Individual dataset performance\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for name, results in real_dataset_results.items():\n",
    "        plt.plot(results['k_values'], results['accuracies'], \n",
    "                marker='o', label=name, linewidth=2)\n",
    "    plt.title('k-NN Performance vs k Value (All Real Datasets)')\n",
    "    plt.xlabel('k Value')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Best accuracy comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    names = list(real_dataset_results.keys())\n",
    "    best_accs = [real_dataset_results[name]['best_accuracy'] for name in names]\n",
    "    bars = plt.bar(names, best_accs, color=plt.cm.Set3(np.linspace(0, 1, len(names))))\n",
    "    plt.title('Best k-NN Accuracy by Dataset')\n",
    "    plt.ylabel('Best Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, best_accs):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Subplot 3: Dataset complexity analysis\n",
    "    plt.subplot(2, 2, 3)\n",
    "    n_samples = [real_dataset_results[name]['n_samples'] for name in names]\n",
    "    n_features = [real_dataset_results[name]['n_features'] for name in names]\n",
    "    \n",
    "    scatter = plt.scatter(n_features, best_accs, s=[s/5 for s in n_samples], \n",
    "                         c=range(len(names)), cmap='viridis', alpha=0.7)\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Best Accuracy')\n",
    "    plt.title('Accuracy vs Dataset Complexity')\n",
    "    \n",
    "    # Add dataset labels\n",
    "    for i, name in enumerate(names):\n",
    "        plt.annotate(name, (n_features[i], best_accs[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # Subplot 4: Optimal k distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    optimal_ks = [real_dataset_results[name]['best_k'] for name in names]\n",
    "    plt.hist(optimal_ks, bins=range(1, max(optimal_ks)+2), alpha=0.7, edgecolor='black')\n",
    "    plt.title('Distribution of Optimal k Values')\n",
    "    plt.xlabel('Optimal k')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(range(1, max(optimal_ks)+1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\nSummary Table - k-NN Performance on Real Datasets:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Dataset':<20} {'Samples':<8} {'Features':<9} {'Best k':<7} {'Best Acc':<10} {'Complexity':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, results in real_dataset_results.items():\n",
    "        complexity = \"High\" if results['n_features'] > 20 else \"Medium\" if results['n_features'] > 10 else \"Low\"\n",
    "        print(f\"{name:<20} {results['n_samples']:<8} {results['n_features']:<9} {results['best_k']:<7} {results['best_accuracy']:<10.4f} {complexity:<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Built-in Datasets Analysis\n",
    "\n",
    "Let's also analyze some built-in scikit-learn datasets for comparison.\n",
    "\n",
    "# Load multiple datasets for comprehensive testing\n",
    "datasets = {\n",
    "    'Iris': load_iris(),\n",
    "    'Wine': load_wine(),\n",
    "    'Breast Cancer': load_breast_cancer()\n",
    "}\n",
    "\n",
    "# Display dataset information\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"\\n{name} Dataset:\")\n",
    "    print(f\"  Samples: {dataset.data.shape[0]}\")\n",
    "    print(f\"  Features: {dataset.data.shape[1]}\")\n",
    "    print(f\"  Classes: {len(dataset.target_names)}\")\n",
    "    print(f\"  Class names: {dataset.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728cea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of Iris dataset\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target\n",
    "iris_df['species'] = iris_df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(\"Iris Dataset Summary:\")\n",
    "print(iris_df.describe())\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Pairplot for first two features\n",
    "sns.scatterplot(data=iris_df, x='sepal length (cm)', y='sepal width (cm)', \n",
    "                hue='species', ax=axes[0,0])\n",
    "axes[0,0].set_title('Sepal Length vs Width')\n",
    "\n",
    "# Pairplot for last two features\n",
    "sns.scatterplot(data=iris_df, x='petal length (cm)', y='petal width (cm)', \n",
    "                hue='species', ax=axes[0,1])\n",
    "axes[0,1].set_title('Petal Length vs Width')\n",
    "\n",
    "# Distribution of features\n",
    "iris_df[iris.feature_names].hist(bins=20, ax=axes[1,0], alpha=0.7)\n",
    "axes[1,0].set_title('Feature Distributions')\n",
    "\n",
    "# Correlation heatmap\n",
    "sns.heatmap(iris_df[iris.feature_names].corr(), annot=True, ax=axes[1,1])\n",
    "axes[1,1].set_title('Feature Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f3a3f",
   "metadata": {},
   "source": [
    "## 5. k-NN Implementation and Testing on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9352341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for k-NN analysis\n",
    "X_train, X_test, y_train, y_test, feature_names, target_names = load_and_prepare_data()\n",
    "\n",
    "# Test our custom k-NN implementation\n",
    "print(\"Testing Custom k-NN Implementation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "knn_custom = KNearestNeighbors(k=5)\n",
    "knn_custom.fit(X_train, y_train)\n",
    "y_pred_custom = knn_custom.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_custom)\n",
    "print(f\"Custom k-NN Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_custom, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0a4e0",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning - Finding Optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different k values\n",
    "k_values = range(1, 21)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Custom implementation\n",
    "    knn = KNearestNeighbors(k=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Train accuracy\n",
    "    y_pred_train = knn.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    # Test accuracy\n",
    "    y_pred_test = knn.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(k_values, train_accuracies, marker='o', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(k_values, test_accuracies, marker='s', label='Testing Accuracy', linewidth=2)\n",
    "plt.title('k-NN Performance: Training vs Testing Accuracy')\n",
    "plt.xlabel('k Value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "\n",
    "# Highlight best k for test accuracy\n",
    "best_k = k_values[np.argmax(test_accuracies)]\n",
    "plt.axvline(x=best_k, color='red', linestyle='--', alpha=0.7, \n",
    "            label=f'Best k = {best_k}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best k value: {best_k} with test accuracy: {max(test_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59dadbe",
   "metadata": {},
   "source": [
    "## 7. Distance Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different distance metrics\n",
    "metrics = ['euclidean', 'manhattan', 'cosine']\n",
    "metric_results = {}\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    # Test with different k values\n",
    "    k_range = range(1, 16)\n",
    "    accuracies = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        knn = KNearestNeighbors(k=k, distance_metric=metric)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    metric_results[metric] = max(accuracies)\n",
    "    \n",
    "    # Plot\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.plot(k_range, accuracies, marker='o', linewidth=2)\n",
    "    plt.title(f'{metric.capitalize()} Distance')\n",
    "    plt.xlabel('k Value')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(0.8, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"Distance Metric Comparison (Best Accuracy):\")\n",
    "for metric, acc in metric_results.items():\n",
    "    print(f\"{metric.capitalize()}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f50be6f",
   "metadata": {},
   "source": [
    "## 8. Comparison with Other Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca8b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare k-NN with other popular algorithms\n",
    "models = {\n",
    "    'k-NN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42)\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "predictions = {}\n",
    "\n",
    "print(\"Model Comparison Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train and predict\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    model_results[name] = accuracy\n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "    print(f\"{name}: {accuracy:.4f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "models_list = list(model_results.keys())\n",
    "accuracies_list = list(model_results.values())\n",
    "\n",
    "bars = plt.bar(models_list, accuracies_list, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.8, 1.05)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies_list):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794cc8de",
   "metadata": {},
   "source": [
    "## 9. High-Dimensional Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on high-dimensional dataset (Breast Cancer - 30 features)\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer, y_cancer = cancer.data, cancer.target\n",
    "\n",
    "print(f\"Breast Cancer Dataset:\")\n",
    "print(f\"Samples: {X_cancer.shape[0]}, Features: {X_cancer.shape[1]}\")\n",
    "print(f\"Classes: {cancer.target_names}\")\n",
    "\n",
    "# Split and scale\n",
    "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.3, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_cancer_scaled = scaler.fit_transform(X_train_cancer)\n",
    "X_test_cancer_scaled = scaler.transform(X_test_cancer)\n",
    "\n",
    "# Test k-NN performance on high-dimensional data\n",
    "k_values = [1, 3, 5, 7, 9, 11, 15, 20]\n",
    "cancer_accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_cancer_scaled, y_train_cancer)\n",
    "    y_pred = knn.predict(X_test_cancer_scaled)\n",
    "    acc = accuracy_score(y_test_cancer, y_pred)\n",
    "    cancer_accuracies.append(acc)\n",
    "    print(f\"k={k}: Accuracy = {acc:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, cancer_accuracies, marker='o', linewidth=2, markersize=8)\n",
    "plt.title('k-NN Performance on High-Dimensional Data (30 features)')\n",
    "plt.xlabel('k Value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "best_k_cancer = k_values[np.argmax(cancer_accuracies)]\n",
    "plt.axvline(x=best_k_cancer, color='red', linestyle='--', alpha=0.7, \n",
    "            label=f'Best k = {best_k_cancer}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738535b0",
   "metadata": {},
   "source": [
    "## 10. Curse of Dimensionality Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e197adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate curse of dimensionality\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "n_samples = 1000\n",
    "n_features_list = [2, 5, 10, 20, 50, 100]\n",
    "dimensionality_results = []\n",
    "\n",
    "print(\"Curse of Dimensionality Analysis:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "for n_features in n_features_list:\n",
    "    # Generate synthetic dataset\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=n_features, \n",
    "                              n_informative=min(n_features, 10), \n",
    "                              n_redundant=0, n_clusters_per_class=1, \n",
    "                              random_state=42)\n",
    "    \n",
    "    # Split and scale\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Test k-NN\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    dimensionality_results.append(accuracy)\n",
    "    print(f\"Features: {n_features:3d}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_features_list, dimensionality_results, marker='o', linewidth=2, markersize=8)\n",
    "plt.title('k-NN Performance vs. Number of Features (Curse of Dimensionality)')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f981362f",
   "metadata": {},
   "source": [
    "## 11. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b059f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation for robust evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Test different k values with cross-validation\n",
    "k_values = range(1, 21)\n",
    "cv_means = []\n",
    "cv_stds = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_means.append(scores.mean())\n",
    "    cv_stds.append(scores.std())\n",
    "\n",
    "# Plot with error bars\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.errorbar(k_values, cv_means, yerr=cv_stds, marker='o', linewidth=2, \n",
    "             capsize=5, capthick=2)\n",
    "plt.title('k-NN Cross-Validation Performance')\n",
    "plt.xlabel('k Value')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "\n",
    "# Find and highlight best k\n",
    "best_k_cv = k_values[np.argmax(cv_means)]\n",
    "plt.axvline(x=best_k_cv, color='red', linestyle='--', alpha=0.7, \n",
    "            label=f'Best k = {best_k_cv}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best k from cross-validation: {best_k_cv}\")\n",
    "print(f\"CV Accuracy: {cv_means[best_k_cv-1]:.4f} ± {cv_stds[best_k_cv-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1cdad",
   "metadata": {},
   "source": [
    "## 12. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "print(\"k-NN ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n1. ALGORITHM CHARACTERISTICS:\")\n",
    "print(\"   • Lazy Learning: No training phase, stores all data\")\n",
    "print(\"   • Instance-based: Uses specific training examples\")\n",
    "print(\"   • Non-parametric: No assumptions about data distribution\")\n",
    "print(\"   • Memory-intensive: Stores entire training set\")\n",
    "\n",
    "print(\"\\n2. DISTANCE METRICS PERFORMANCE:\")\n",
    "for metric, acc in metric_results.items():\n",
    "    print(f\"   • {metric.capitalize()}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\n3. MODEL COMPARISON:\")\n",
    "for model, acc in model_results.items():\n",
    "    print(f\"   • {model}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\n4. OPTIMAL HYPERPARAMETERS:\")\n",
    "print(f\"   • Best k (simple): {best_k}\")\n",
    "print(f\"   • Best k (CV): {best_k_cv}\")\n",
    "print(f\"   • Best distance metric: {max(metric_results.items(), key=lambda x: x[1])[0]}\")\n",
    "\n",
    "print(\"\\n5. HIGH-DIMENSIONAL PERFORMANCE:\")\n",
    "print(f\"   • Best accuracy on 30D data: {max(cancer_accuracies):.4f}\")\n",
    "print(f\"   • Performance degrades with increasing dimensions\")\n",
    "\n",
    "print(\"\\n6. KEY INSIGHTS:\")\n",
    "print(\"   • k-NN works well on low-dimensional, well-separated data\")\n",
    "print(\"   • Sensitive to curse of dimensionality\")\n",
    "print(\"   • Feature scaling is crucial for distance-based algorithms\")\n",
    "print(\"   • Choice of k affects bias-variance tradeoff\")\n",
    "print(\"   • Computationally expensive at prediction time\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
