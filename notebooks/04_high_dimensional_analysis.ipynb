{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e6019e",
   "metadata": {},
   "source": [
    "# 4. k-NN on High-Dimensional Data & Curse of Dimensionality\n",
    "\n",
    "## Overview\n",
    "This notebook explores:\n",
    "- How k-NN performs on high-dimensional datasets\n",
    "- The curse of dimensionality phenomenon\n",
    "- Why distance metrics become less meaningful in high dimensions\n",
    "- Mitigation strategies for high-dimensional data\n",
    "- Practical implications and solutions\n",
    "- Analysis using the banknote dataset with synthetic high-dimensional extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeaadf2",
   "metadata": {},
   "source": [
    "## Understanding the Curse of Dimensionality\n",
    "\n",
    "### What is the Curse of Dimensionality?\n",
    "\n",
    "The curse of dimensionality refers to various phenomena that arise when analyzing data in high-dimensional spaces. For k-NN specifically:\n",
    "\n",
    "1. **Distance Concentration**: In high dimensions, distances between points become similar\n",
    "2. **Sparse Data**: Data points become sparse and isolated\n",
    "3. **Nearest Neighbors Become Less \"Near\"**: The concept of \"closeness\" loses meaning\n",
    "4. **Computational Complexity**: Distance calculations become expensive\n",
    "\n",
    "### Mathematical Intuition\n",
    "\n",
    "In d-dimensional space:\n",
    "- **Volume grows exponentially**: V ‚àù r^d\n",
    "- **Surface area dominates**: Most points lie near the boundary\n",
    "- **Distance ratio converges**: max_dist/min_dist ‚Üí 1 as d ‚Üí ‚àû\n",
    "\n",
    "### Why This Affects k-NN\n",
    "\n",
    "k-NN relies on the assumption that **nearby points are similar**. When this assumption breaks down:\n",
    "- All points appear equally \"close\"\n",
    "- Neighbors become random\n",
    "- Classification becomes unreliable\n",
    "- Performance degrades significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994316e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ecc9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prepared banknote data\n",
    "with open('../data/prepared_banknote_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train_orig = data['X_train']\n",
    "X_test_orig = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "X_train_scaled_orig = data['X_train_scaled']\n",
    "X_test_scaled_orig = data['X_test_scaled']\n",
    "scaler = data['scaler']\n",
    "feature_names = data['feature_names']\n",
    "\n",
    "print(\"Original banknote data loaded successfully!\")\n",
    "print(f\"Original dimensions: {X_train_scaled_orig.shape[1]} features\")\n",
    "print(f\"Training samples: {X_train_scaled_orig.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_scaled_orig.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbfe295",
   "metadata": {},
   "source": [
    "## Demonstrating Distance Concentration\n",
    "\n",
    "Let's create synthetic high-dimensional versions of our banknote data to show how distances behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ea7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create high-dimensional versions of the banknote dataset\n",
    "def create_high_dimensional_data(X_orig, dimensions_list, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Create high-dimensional versions by adding noise features\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    n_orig_features = X_orig.shape[1]\n",
    "    \n",
    "    for d in dimensions_list:\n",
    "        if d <= n_orig_features:\n",
    "            # Use subset of original features\n",
    "            X_high_dim = X_orig[:, :d]\n",
    "        else:\n",
    "            # Add noise features\n",
    "            n_noise_features = d - n_orig_features\n",
    "            noise_features = np.random.normal(0, noise_std, \n",
    "                                            (X_orig.shape[0], n_noise_features))\n",
    "            X_high_dim = np.hstack([X_orig, noise_features])\n",
    "        \n",
    "        datasets[d] = X_high_dim\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Create datasets with different dimensionalities\n",
    "dimensions = [2, 4, 8, 16, 32, 64, 128]\n",
    "train_datasets = create_high_dimensional_data(X_train_scaled_orig, dimensions)\n",
    "test_datasets = create_high_dimensional_data(X_test_scaled_orig, dimensions)\n",
    "\n",
    "print(\"High-dimensional datasets created:\")\n",
    "for d in dimensions:\n",
    "    print(f\"  {d}D: {train_datasets[d].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149381e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distance distribution in different dimensions\n",
    "def analyze_distances(X, n_samples=100):\n",
    "    \"\"\"\n",
    "    Analyze pairwise distance distribution\n",
    "    \"\"\"\n",
    "    # Sample random points to avoid computational explosion\n",
    "    if len(X) > n_samples:\n",
    "        indices = np.random.choice(len(X), n_samples, replace=False)\n",
    "        X_sample = X[indices]\n",
    "    else:\n",
    "        X_sample = X\n",
    "    \n",
    "    # Calculate all pairwise distances\n",
    "    distances = []\n",
    "    for i in range(len(X_sample)):\n",
    "        for j in range(i+1, len(X_sample)):\n",
    "            dist = np.sqrt(np.sum((X_sample[i] - X_sample[j])**2))\n",
    "            distances.append(dist)\n",
    "    \n",
    "    distances = np.array(distances)\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(distances),\n",
    "        'std': np.std(distances),\n",
    "        'min': np.min(distances),\n",
    "        'max': np.max(distances),\n",
    "        'cv': np.std(distances) / np.mean(distances),  # Coefficient of variation\n",
    "        'ratio': np.max(distances) / np.min(distances),  # Max/min ratio\n",
    "        'distances': distances\n",
    "    }\n",
    "\n",
    "# Analyze distance statistics for each dimension\n",
    "distance_stats = {}\n",
    "print(\"DISTANCE CONCENTRATION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Dim':<4} {'Mean':<8} {'Std':<8} {'CV':<8} {'Max/Min':<8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for d in dimensions:\n",
    "    stats = analyze_distances(train_datasets[d])\n",
    "    distance_stats[d] = stats\n",
    "    \n",
    "    print(f\"{d:<4} {stats['mean']:<8.3f} {stats['std']:<8.3f} {stats['cv']:<8.3f} {stats['ratio']:<8.3f}\")\n",
    "\n",
    "print(\"\\nCV = Coefficient of Variation (lower = more concentrated)\")\n",
    "print(\"Max/Min = Ratio of maximum to minimum distance (lower = more concentrated)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distance concentration\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Distance Concentration in High Dimensions', fontsize=16)\n",
    "\n",
    "# 1. Distance distributions for different dimensions\n",
    "selected_dims = [2, 8, 32, 128]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "for i, (d, color) in enumerate(zip(selected_dims, colors)):\n",
    "    distances = distance_stats[d]['distances']\n",
    "    axes[0, 0].hist(distances, bins=30, alpha=0.6, label=f'{d}D', \n",
    "                   color=color, density=True)\n",
    "\n",
    "axes[0, 0].set_title('Distance Distributions by Dimension')\n",
    "axes[0, 0].set_xlabel('Euclidean Distance')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Coefficient of variation vs dimension\n",
    "dims = list(distance_stats.keys())\n",
    "cvs = [distance_stats[d]['cv'] for d in dims]\n",
    "\n",
    "axes[0, 1].plot(dims, cvs, 'o-', linewidth=2, markersize=8, color='red')\n",
    "axes[0, 1].set_title('Distance Concentration\\n(Lower CV = More Concentrated)')\n",
    "axes[0, 1].set_xlabel('Number of Dimensions')\n",
    "axes[0, 1].set_ylabel('Coefficient of Variation')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xscale('log')\n",
    "\n",
    "# 3. Max/Min distance ratio\n",
    "ratios = [distance_stats[d]['ratio'] for d in dims]\n",
    "axes[1, 0].plot(dims, ratios, 's-', linewidth=2, markersize=8, color='blue')\n",
    "axes[1, 0].set_title('Distance Range Compression\\n(Lower Ratio = More Compressed)')\n",
    "axes[1, 0].set_xlabel('Number of Dimensions')\n",
    "axes[1, 0].set_ylabel('Max Distance / Min Distance')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xscale('log')\n",
    "\n",
    "# 4. Mean distance vs dimension\n",
    "mean_dists = [distance_stats[d]['mean'] for d in dims]\n",
    "axes[1, 1].plot(dims, mean_dists, '^-', linewidth=2, markersize=8, color='green')\n",
    "axes[1, 1].set_title('Mean Distance vs Dimensionality')\n",
    "axes[1, 1].set_xlabel('Number of Dimensions')\n",
    "axes[1, 1].set_ylabel('Mean Euclidean Distance')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° KEY OBSERVATIONS:\")\n",
    "print(\"‚Ä¢ Coefficient of Variation decreases ‚Üí distances become more similar\")\n",
    "print(\"‚Ä¢ Max/Min ratio decreases ‚Üí range of distances compresses\")\n",
    "print(\"‚Ä¢ Mean distance increases ‚Üí but variance doesn't keep up\")\n",
    "print(\"‚Ä¢ This is the curse of dimensionality in action!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb5847",
   "metadata": {},
   "source": [
    "## k-NN Performance vs Dimensionality\n",
    "\n",
    "Now let's see how k-NN classification performance degrades as dimensionality increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d01e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test k-NN performance across dimensions\n",
    "def evaluate_knn_performance(train_datasets, test_datasets, y_train, y_test, k=5):\n",
    "    \"\"\"\n",
    "    Evaluate k-NN performance across different dimensions\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for d in train_datasets.keys():\n",
    "        # Train k-NN\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(train_datasets[d], y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = knn.predict(test_datasets[d])\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Calculate prediction confidence (distance to nearest neighbors)\n",
    "        distances, indices = knn.kneighbors(test_datasets[d])\n",
    "        avg_neighbor_distance = np.mean(distances)\n",
    "        \n",
    "        results[d] = {\n",
    "            'accuracy': accuracy,\n",
    "            'avg_neighbor_distance': avg_neighbor_distance,\n",
    "            'dimensions': d\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate performance\n",
    "performance_results = evaluate_knn_performance(train_datasets, test_datasets, \n",
    "                                              y_train, y_test, k=5)\n",
    "\n",
    "print(\"k-NN PERFORMANCE VS DIMENSIONALITY\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"{'Dimensions':<12} {'Accuracy':<10} {'Avg Neighbor Dist':<18}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for d, results in performance_results.items():\n",
    "    print(f\"{d:<12} {results['accuracy']:<10.4f} {results['avg_neighbor_distance']:<18.4f}\")\n",
    "\n",
    "# Find performance degradation point\n",
    "accuracies = [performance_results[d]['accuracy'] for d in dimensions]\n",
    "max_accuracy = max(accuracies)\n",
    "degradation_threshold = max_accuracy * 0.95  # 5% drop\n",
    "\n",
    "degradation_point = None\n",
    "for d, acc in zip(dimensions, accuracies):\n",
    "    if acc < degradation_threshold:\n",
    "        degradation_point = d\n",
    "        break\n",
    "\n",
    "if degradation_point:\n",
    "    print(f\"\\n‚ö†Ô∏è  Performance starts degrading at {degradation_point} dimensions\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Performance remains stable across all tested dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance degradation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('k-NN Performance Degradation in High Dimensions', fontsize=16)\n",
    "\n",
    "dims = list(performance_results.keys())\n",
    "accuracies = [performance_results[d]['accuracy'] for d in dims]\n",
    "neighbor_dists = [performance_results[d]['avg_neighbor_distance'] for d in dims]\n",
    "\n",
    "# 1. Accuracy vs Dimensions\n",
    "axes[0, 0].plot(dims, accuracies, 'o-', linewidth=3, markersize=8, color='red')\n",
    "axes[0, 0].set_title('Classification Accuracy vs Dimensions')\n",
    "axes[0, 0].set_xlabel('Number of Dimensions')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xscale('log')\n",
    "axes[0, 0].set_ylim(0.5, 1.05)\n",
    "\n",
    "# Add horizontal line for random performance\n",
    "axes[0, 0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, \n",
    "                  label='Random Performance')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Average neighbor distance vs Dimensions\n",
    "axes[0, 1].plot(dims, neighbor_dists, 's-', linewidth=3, markersize=8, color='blue')\n",
    "axes[0, 1].set_title('Average Distance to Nearest Neighbors')\n",
    "axes[0, 1].set_xlabel('Number of Dimensions')\n",
    "axes[0, 1].set_ylabel('Average Distance')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xscale('log')\n",
    "\n",
    "# 3. Accuracy vs Neighbor Distance (correlation)\n",
    "axes[1, 0].scatter(neighbor_dists, accuracies, s=100, alpha=0.7, c=dims, \n",
    "                  cmap='viridis')\n",
    "axes[1, 0].set_title('Accuracy vs Average Neighbor Distance')\n",
    "axes[1, 0].set_xlabel('Average Neighbor Distance')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add colorbar\n",
    "scatter = axes[1, 0].scatter(neighbor_dists, accuracies, s=100, alpha=0.7, \n",
    "                           c=dims, cmap='viridis')\n",
    "cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
    "cbar.set_label('Dimensions')\n",
    "\n",
    "# 4. Performance degradation rate\n",
    "# Calculate relative performance (normalized to best performance)\n",
    "relative_performance = [acc / max(accuracies) for acc in accuracies]\n",
    "axes[1, 1].plot(dims, relative_performance, '^-', linewidth=3, markersize=8, color='orange')\n",
    "axes[1, 1].set_title('Relative Performance Degradation')\n",
    "axes[1, 1].set_xlabel('Number of Dimensions')\n",
    "axes[1, 1].set_ylabel('Relative Performance')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xscale('log')\n",
    "axes[1, 1].set_ylim(0.5, 1.05)\n",
    "\n",
    "# Add threshold lines\n",
    "axes[1, 1].axhline(y=0.95, color='orange', linestyle='--', alpha=0.7, \n",
    "                  label='5% degradation')\n",
    "axes[1, 1].axhline(y=0.90, color='red', linestyle='--', alpha=0.7, \n",
    "                  label='10% degradation')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate performance drop statistics\n",
    "performance_drop = (max(accuracies) - min(accuracies)) / max(accuracies) * 100\n",
    "print(f\"\\nüìâ PERFORMANCE DEGRADATION SUMMARY:\")\n",
    "print(f\"‚Ä¢ Best accuracy: {max(accuracies):.4f} at {dims[accuracies.index(max(accuracies))]} dimensions\")\n",
    "print(f\"‚Ä¢ Worst accuracy: {min(accuracies):.4f} at {dims[accuracies.index(min(accuracies))]} dimensions\")\n",
    "print(f\"‚Ä¢ Total performance drop: {performance_drop:.1f}%\")\n",
    "print(f\"‚Ä¢ Average neighbor distance increases {neighbor_dists[-1]/neighbor_dists[0]:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c5f28",
   "metadata": {},
   "source": [
    "## Mitigation Strategies for High-Dimensional Data\n",
    "\n",
    "While the curse of dimensionality is a fundamental challenge, there are several strategies to mitigate its effects:\n",
    "\n",
    "### 1. Dimensionality Reduction\n",
    "- **Principal Component Analysis (PCA)**: Projects data to lower dimensions\n",
    "- **Feature Selection**: Choose most relevant features\n",
    "- **Linear Discriminant Analysis (LDA)**: Supervised dimensionality reduction\n",
    "\n",
    "### 2. Feature Engineering\n",
    "- Remove irrelevant/noisy features\n",
    "- Create meaningful feature combinations\n",
    "- Domain-specific feature selection\n",
    "\n",
    "### 3. Algorithm Modifications\n",
    "- Use different distance metrics\n",
    "- Weighted k-NN (weight by distance)\n",
    "- Local distance metrics\n",
    "\n",
    "Let's test these strategies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ebaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Principal Component Analysis (PCA)\n",
    "def test_pca_strategy(X_train_high_dim, X_test_high_dim, y_train, y_test, \n",
    "                     n_components_list, original_performance):\n",
    "    \"\"\"\n",
    "    Test PCA dimensionality reduction strategy\n",
    "    \"\"\"\n",
    "    pca_results = {}\n",
    "    \n",
    "    print(f\"\\nTesting PCA on {X_train_high_dim.shape[1]}D data:\")\n",
    "    print(f\"Original performance: {original_performance:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for n_comp in n_components_list:\n",
    "        if n_comp >= X_train_high_dim.shape[1]:\n",
    "            continue\n",
    "            \n",
    "        # Apply PCA\n",
    "        pca = PCA(n_components=n_comp)\n",
    "        X_train_pca = pca.fit_transform(X_train_high_dim)\n",
    "        X_test_pca = pca.transform(X_test_high_dim)\n",
    "        \n",
    "        # Train k-NN on reduced data\n",
    "        knn = KNeighborsClassifier(n_neighbors=5)\n",
    "        knn.fit(X_train_pca, y_train)\n",
    "        y_pred = knn.predict(X_test_pca)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Calculate explained variance\n",
    "        explained_var = np.sum(pca.explained_variance_ratio_)\n",
    "        \n",
    "        pca_results[n_comp] = {\n",
    "            'accuracy': accuracy,\n",
    "            'explained_variance': explained_var,\n",
    "            'improvement': accuracy - original_performance\n",
    "        }\n",
    "        \n",
    "        print(f\"PCA to {n_comp:2d}D: Acc={accuracy:.4f} (+{accuracy-original_performance:+.4f}), \"\n",
    "              f\"Var Explained={explained_var:.3f}\")\n",
    "    \n",
    "    return pca_results\n",
    "\n",
    "# Test PCA on our highest dimensional dataset (128D)\n",
    "high_dim = 128\n",
    "original_perf = performance_results[high_dim]['accuracy']\n",
    "\n",
    "pca_components = [2, 4, 8, 16, 32]\n",
    "pca_results = test_pca_strategy(train_datasets[high_dim], test_datasets[high_dim],\n",
    "                               y_train, y_test, pca_components, original_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e87bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Feature Selection\n",
    "def test_feature_selection_strategy(X_train_high_dim, X_test_high_dim, y_train, y_test,\n",
    "                                   k_features_list, original_performance):\n",
    "    \"\"\"\n",
    "    Test feature selection strategy using univariate statistical tests\n",
    "    \"\"\"\n",
    "    fs_results = {}\n",
    "    \n",
    "    print(f\"\\nTesting Feature Selection on {X_train_high_dim.shape[1]}D data:\")\n",
    "    print(f\"Original performance: {original_performance:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for k_features in k_features_list:\n",
    "        if k_features >= X_train_high_dim.shape[1]:\n",
    "            continue\n",
    "            \n",
    "        # Apply feature selection\n",
    "        selector = SelectKBest(score_func=f_classif, k=k_features)\n",
    "        X_train_selected = selector.fit_transform(X_train_high_dim, y_train)\n",
    "        X_test_selected = selector.transform(X_test_high_dim)\n",
    "        \n",
    "        # Train k-NN on selected features\n",
    "        knn = KNeighborsClassifier(n_neighbors=5)\n",
    "        knn.fit(X_train_selected, y_train)\n",
    "        y_pred = knn.predict(X_test_selected)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Get feature importance scores\n",
    "        feature_scores = selector.scores_\n",
    "        selected_features = selector.get_support()\n",
    "        avg_score = np.mean(feature_scores[selected_features])\n",
    "        \n",
    "        fs_results[k_features] = {\n",
    "            'accuracy': accuracy,\n",
    "            'avg_feature_score': avg_score,\n",
    "            'improvement': accuracy - original_performance,\n",
    "            'selected_features': selected_features\n",
    "        }\n",
    "        \n",
    "        print(f\"Select {k_features:2d} features: Acc={accuracy:.4f} (+{accuracy-original_performance:+.4f}), \"\n",
    "              f\"Avg Score={avg_score:.2f}\")\n",
    "    \n",
    "    return fs_results\n",
    "\n",
    "# Test feature selection\n",
    "feature_counts = [2, 4, 8, 16, 32]\n",
    "fs_results = test_feature_selection_strategy(train_datasets[high_dim], test_datasets[high_dim],\n",
    "                                           y_train, y_test, feature_counts, original_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe09324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Weighted k-NN\n",
    "def test_weighted_knn_strategy(X_train, X_test, y_train, y_test, k_values):\n",
    "    \"\"\"\n",
    "    Test weighted k-NN (distance-weighted) vs uniform k-NN\n",
    "    \"\"\"\n",
    "    print(f\"\\nTesting Weighted k-NN vs Uniform k-NN:\")\n",
    "    print(\"-\" * 45)\n",
    "    print(f\"{'k':<3} {'Uniform':<10} {'Weighted':<10} {'Improvement':<12}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    weighted_results = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        # Uniform weights\n",
    "        knn_uniform = KNeighborsClassifier(n_neighbors=k, weights='uniform')\n",
    "        knn_uniform.fit(X_train, y_train)\n",
    "        y_pred_uniform = knn_uniform.predict(X_test)\n",
    "        acc_uniform = accuracy_score(y_test, y_pred_uniform)\n",
    "        \n",
    "        # Distance weights\n",
    "        knn_weighted = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "        knn_weighted.fit(X_train, y_train)\n",
    "        y_pred_weighted = knn_weighted.predict(X_test)\n",
    "        acc_weighted = accuracy_score(y_test, y_pred_weighted)\n",
    "        \n",
    "        improvement = acc_weighted - acc_uniform\n",
    "        \n",
    "        weighted_results[k] = {\n",
    "            'uniform': acc_uniform,\n",
    "            'weighted': acc_weighted,\n",
    "            'improvement': improvement\n",
    "        }\n",
    "        \n",
    "        print(f\"{k:<3} {acc_uniform:<10.4f} {acc_weighted:<10.4f} {improvement:<+12.4f}\")\n",
    "    \n",
    "    return weighted_results\n",
    "\n",
    "# Test weighted k-NN on high-dimensional data\n",
    "k_values_test = [3, 5, 7, 9, 11]\n",
    "weighted_results = test_weighted_knn_strategy(train_datasets[high_dim], test_datasets[high_dim],\n",
    "                                            y_train, y_test, k_values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9025055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize mitigation strategies\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Mitigation Strategies for High-Dimensional k-NN', fontsize=16)\n",
    "\n",
    "# 1. PCA Results\n",
    "pca_dims = list(pca_results.keys())\n",
    "pca_accuracies = [pca_results[d]['accuracy'] for d in pca_dims]\n",
    "pca_variances = [pca_results[d]['explained_variance'] for d in pca_dims]\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "line1 = ax1.plot(pca_dims, pca_accuracies, 'o-', color='blue', linewidth=2, label='Accuracy')\n",
    "ax1.axhline(y=original_perf, color='red', linestyle='--', alpha=0.7, label=f'Original ({original_perf:.3f})')\n",
    "ax1.set_xlabel('PCA Components')\n",
    "ax1.set_ylabel('Accuracy', color='blue')\n",
    "ax1.set_title('PCA Dimensionality Reduction')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add variance explained on secondary y-axis\n",
    "ax1_twin = ax1.twinx()\n",
    "line2 = ax1_twin.plot(pca_dims, pca_variances, 's-', color='green', linewidth=2, label='Explained Variance')\n",
    "ax1_twin.set_ylabel('Explained Variance', color='green')\n",
    "\n",
    "# Combine legends\n",
    "lines = line1 + line2 + [plt.Line2D([0], [0], color='red', linestyle='--', label=f'Original ({original_perf:.3f})')]\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='lower right')\n",
    "\n",
    "# 2. Feature Selection Results\n",
    "fs_dims = list(fs_results.keys())\n",
    "fs_accuracies = [fs_results[d]['accuracy'] for d in fs_dims]\n",
    "fs_scores = [fs_results[d]['avg_feature_score'] for d in fs_dims]\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "line3 = ax2.plot(fs_dims, fs_accuracies, 'o-', color='purple', linewidth=2, label='Accuracy')\n",
    "ax2.axhline(y=original_perf, color='red', linestyle='--', alpha=0.7, label=f'Original ({original_perf:.3f})')\n",
    "ax2.set_xlabel('Selected Features')\n",
    "ax2.set_ylabel('Accuracy', color='purple')\n",
    "ax2.set_title('Feature Selection (SelectKBest)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Weighted vs Uniform k-NN\n",
    "k_vals = list(weighted_results.keys())\n",
    "uniform_accs = [weighted_results[k]['uniform'] for k in k_vals]\n",
    "weighted_accs = [weighted_results[k]['weighted'] for k in k_vals]\n",
    "\n",
    "axes[1, 0].plot(k_vals, uniform_accs, 'o-', label='Uniform Weights', linewidth=2, color='orange')\n",
    "axes[1, 0].plot(k_vals, weighted_accs, 's-', label='Distance Weights', linewidth=2, color='brown')\n",
    "axes[1, 0].set_xlabel('k Value')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].set_title('Weighted vs Uniform k-NN')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Strategy Comparison\n",
    "strategies = ['Original\\n(128D)', 'Best PCA', 'Best FS', 'Best Weighted']\n",
    "best_pca_acc = max(pca_accuracies)\n",
    "best_fs_acc = max(fs_accuracies)\n",
    "best_weighted_acc = max([max(uniform_accs), max(weighted_accs)])\n",
    "\n",
    "strategy_accs = [original_perf, best_pca_acc, best_fs_acc, best_weighted_acc]\n",
    "colors = ['red', 'blue', 'purple', 'brown']\n",
    "\n",
    "bars = axes[1, 1].bar(strategies, strategy_accs, color=colors, alpha=0.7)\n",
    "axes[1, 1].set_title('Strategy Comparison\\n(Best Performance)')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].set_ylim(0.5, 1.05)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, strategy_accs):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                   f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of best strategies\n",
    "print(\"\\nüèÜ MITIGATION STRATEGY RESULTS:\")\n",
    "print(f\"‚Ä¢ Original 128D performance: {original_perf:.4f}\")\n",
    "print(f\"‚Ä¢ Best PCA improvement: +{best_pca_acc - original_perf:.4f} (PCA to {pca_dims[pca_accuracies.index(best_pca_acc)]}D)\")\n",
    "print(f\"‚Ä¢ Best Feature Selection: +{best_fs_acc - original_perf:.4f} (Select {fs_dims[fs_accuracies.index(best_fs_acc)]} features)\")\n",
    "print(f\"‚Ä¢ Best Weighted k-NN: +{best_weighted_acc - original_perf:.4f}\")\n",
    "print(f\"‚Ä¢ Best overall strategy: {strategies[strategy_accs.index(max(strategy_accs))]} ({max(strategy_accs):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b6b93",
   "metadata": {},
   "source": [
    "## Practical Recommendations for High-Dimensional k-NN\n",
    "\n",
    "Based on our analysis, here are practical guidelines for using k-NN with high-dimensional data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec5711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive recommendation system\n",
    "def analyze_dataset_and_recommend(X, y, max_dims_threshold=20):\n",
    "    \"\"\"\n",
    "    Analyze dataset characteristics and provide k-NN recommendations\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_classes = len(np.unique(y))\n",
    "    \n",
    "    print(\"DATASET ANALYSIS & k-NN RECOMMENDATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Dataset characteristics:\")\n",
    "    print(f\"  ‚Ä¢ Samples: {n_samples:,}\")\n",
    "    print(f\"  ‚Ä¢ Features: {n_features}\")\n",
    "    print(f\"  ‚Ä¢ Classes: {n_classes}\")\n",
    "    print(f\"  ‚Ä¢ Samples per feature: {n_samples/n_features:.1f}\")\n",
    "    \n",
    "    # Dimensionality assessment\n",
    "    if n_features <= 5:\n",
    "        dim_category = \"Low\"\n",
    "        dim_risk = \"‚úÖ Minimal\"\n",
    "    elif n_features <= 20:\n",
    "        dim_category = \"Medium\"\n",
    "        dim_risk = \"‚ö†Ô∏è  Moderate\"\n",
    "    else:\n",
    "        dim_category = \"High\"\n",
    "        dim_risk = \"üö® High\"\n",
    "    \n",
    "    print(f\"\\nDimensionality Assessment:\")\n",
    "    print(f\"  ‚Ä¢ Category: {dim_category} dimensional\")\n",
    "    print(f\"  ‚Ä¢ Curse of dimensionality risk: {dim_risk}\")\n",
    "    \n",
    "    # Sample density assessment\n",
    "    sample_density = n_samples / (n_features ** 2)  # Rough heuristic\n",
    "    if sample_density > 10:\n",
    "        density_status = \"‚úÖ Good\"\n",
    "    elif sample_density > 1:\n",
    "        density_status = \"‚ö†Ô∏è  Moderate\"\n",
    "    else:\n",
    "        density_status = \"üö® Poor\"\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Sample density: {density_status} ({sample_density:.2f})\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüìã RECOMMENDATIONS:\")\n",
    "    \n",
    "    if n_features <= max_dims_threshold:\n",
    "        print(\"  ‚úÖ k-NN is suitable for this dataset\")\n",
    "        print(\"  ‚Ä¢ Use standard k-NN with proper feature scaling\")\n",
    "        print(\"  ‚Ä¢ Try k values between 3-11\")\n",
    "        print(\"  ‚Ä¢ Consider cross-validation for optimal k\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è  High-dimensional data detected - apply mitigation:\")\n",
    "        print(\"  1. üéØ Dimensionality Reduction:\")\n",
    "        print(f\"     ‚Ä¢ Try PCA to ~{min(10, n_features//4)} components\")\n",
    "        print(f\"     ‚Ä¢ Consider feature selection (top {min(15, n_features//3)} features)\")\n",
    "        print(\"  2. üîß Algorithm Modifications:\")\n",
    "        print(\"     ‚Ä¢ Use distance-weighted k-NN\")\n",
    "        print(\"     ‚Ä¢ Try larger k values (5-15)\")\n",
    "        print(\"     ‚Ä¢ Consider alternative distance metrics\")\n",
    "        print(\"  3. üîÑ Alternative Approaches:\")\n",
    "        print(\"     ‚Ä¢ Consider ensemble methods (Random Forest)\")\n",
    "        print(\"     ‚Ä¢ Try SVM with RBF kernel\")\n",
    "        print(\"     ‚Ä¢ Consider neural networks for complex patterns\")\n",
    "    \n",
    "    # Feature scaling reminder\n",
    "    print(\"\\n  üîß Always Remember:\")\n",
    "    print(\"     ‚Ä¢ Feature scaling is CRITICAL for k-NN\")\n",
    "    print(\"     ‚Ä¢ Use StandardScaler or MinMaxScaler\")\n",
    "    print(\"     ‚Ä¢ Handle missing values appropriately\")\n",
    "    print(\"     ‚Ä¢ Consider feature engineering for domain-specific improvements\")\n",
    "    \n",
    "    return {\n",
    "        'dimensionality_category': dim_category,\n",
    "        'recommended_suitable': n_features <= max_dims_threshold,\n",
    "        'sample_density': sample_density\n",
    "    }\n",
    "\n",
    "# Analyze our banknote dataset\n",
    "analysis = analyze_dataset_and_recommend(X_train_scaled_orig, y_train)\n",
    "\n",
    "# Analyze high-dimensional version\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "analysis_high_dim = analyze_dataset_and_recommend(train_datasets[128], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1459e4",
   "metadata": {},
   "source": [
    "## Summary: k-NN and High-Dimensional Data\n",
    "\n",
    "### Key Findings from Our Analysis:\n",
    "\n",
    "1. **Curse of Dimensionality is Real**\n",
    "   - Distance concentration occurs as dimensions increase\n",
    "   - Nearest neighbors become less \"meaningful\"\n",
    "   - Performance degrades significantly in high dimensions\n",
    "\n",
    "2. **Performance Degradation Patterns**\n",
    "   - Gradual decline in accuracy with increasing dimensions\n",
    "   - Increased average distance to nearest neighbors\n",
    "   - Loss of discriminative power\n",
    "\n",
    "3. **Effective Mitigation Strategies**\n",
    "   - **PCA**: Reduces dimensions while preserving variance\n",
    "   - **Feature Selection**: Keeps only most informative features\n",
    "   - **Distance Weighting**: Gives more weight to closer neighbors\n",
    "\n",
    "4. **Practical Guidelines**\n",
    "   - ‚úÖ k-NN works well: ‚â§20 dimensions with good sample density\n",
    "   - ‚ö†Ô∏è Apply mitigation: 20-100 dimensions\n",
    "   - üö® Consider alternatives: >100 dimensions\n",
    "\n",
    "### Best Practices for High-Dimensional k-NN:\n",
    "\n",
    "1. **Always scale features** (StandardScaler/MinMaxScaler)\n",
    "2. **Apply dimensionality reduction** when d > 20\n",
    "3. **Use distance-weighted k-NN** for better performance\n",
    "4. **Validate with cross-validation** to avoid overfitting\n",
    "5. **Consider ensemble methods** as alternatives\n",
    "\n",
    "### When to Avoid k-NN:\n",
    "- Very high dimensions (>100) without effective reduction\n",
    "- Sparse, high-dimensional data (text, genomics)\n",
    "- When computational efficiency is critical\n",
    "- When interpretability of individual features is needed\n",
    "\n",
    "### Alternative Algorithms for High-Dimensional Data:\n",
    "- **Random Forest**: Handles high dimensions well\n",
    "- **SVM with RBF kernel**: Good for complex boundaries\n",
    "- **Neural Networks**: Can learn complex patterns\n",
    "- **Naive Bayes**: Surprisingly effective for high-dimensional text data\n",
    "\n",
    "The curse of dimensionality is a fundamental challenge, but with proper understanding and mitigation strategies, k-NN can still be effective in many high-dimensional scenarios!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
